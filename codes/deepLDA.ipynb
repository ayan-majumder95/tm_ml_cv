{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fRAjy-Duxtu",
    "outputId": "95042aa4-d4b2-4375-f07c-41b8e29d89fb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import progressbar\n",
    "\n",
    "print(\"Using Pytorch\",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "FdgPU6ZzgtWZ"
   },
   "outputs": [],
   "source": [
    "#@title Define NN architecture and loss function\n",
    "\n",
    "##################################\n",
    "# Custom Dataset\n",
    "##################################\n",
    "class ColvarDataset(Dataset):\n",
    "    \"\"\"COLVAR dataset\"\"\"\n",
    "\n",
    "    def __init__(self, colvar_list):\n",
    "        self.nstates = len( colvar_list )\n",
    "        self.colvar = colvar_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.colvar[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = ()\n",
    "        for i in range(self.nstates):\n",
    "            x += (self.colvar[i][idx],)\n",
    "        return x\n",
    "\n",
    "#useful for cycling over the test dataset if it is smaller than the training set\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "##################################\n",
    "# Define Networks\n",
    "##################################\n",
    "\n",
    "class NN_DeepLDA(nn.Module):\n",
    "\n",
    "    def __init__(self, l ):\n",
    "        super(NN_DeepLDA, self).__init__()\n",
    "\n",
    "        modules=[]\n",
    "        for i in range( len(l)-1 ):\n",
    "            print(l[i],' --> ', l[i+1], end=' ')\n",
    "            if( i<len(l)-2 ):\n",
    "                modules.append(nn.Linear(l[i], l[i+1]) )\n",
    "                modules.append( nn.ReLU(True) )\n",
    "                print(\"(relu)\")\n",
    "            else:\n",
    "                modules.append(nn.Linear(l[i], l[i+1]) )\n",
    "                print(\"\")\n",
    "\n",
    "        self.nn = nn.Sequential(*modules)\n",
    "\n",
    "        #norm option\n",
    "        self.normIn = False\n",
    "\n",
    "    def set_norm(self, Mean: torch.Tensor, Range: torch.Tensor):\n",
    "        self.normIn = True\n",
    "        self.Mean = Mean\n",
    "        self.Range = Range\n",
    "\n",
    "    def normalize(self, x: Variable):\n",
    "        batch_size = x.size(0)\n",
    "        x_size = x.size(1)\n",
    "\n",
    "        Mean = self.Mean.unsqueeze(0).expand(batch_size, x_size)\n",
    "        Range = self.Range.unsqueeze(0).expand(batch_size, x_size)\n",
    "\n",
    "        return x.sub(Mean).div(Range)\n",
    "\n",
    "    def get_hidden(self, x: Variable, svd=False, svd_vectors=False, svd_eigen=False, training=False) -> (Variable):\n",
    "        if(self.normIn):\n",
    "            x = self.normalize(x)\n",
    "        z = self.nn(x)\n",
    "        return z\n",
    "\n",
    "    def set_lda(self, x: torch.Tensor):\n",
    "        self.lda = nn.Parameter(x.unsqueeze(0), requires_grad=False)\n",
    "\n",
    "    def get_lda(self) -> (torch.Tensor):\n",
    "        return self.lda\n",
    "\n",
    "    def apply_lda(self, x: Variable) -> (Variable):\n",
    "        z = torch.nn.functional.linear(x,self.lda)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x: Variable) -> (Variable):\n",
    "        z = self.get_hidden(x,svd=False)\n",
    "        z = self.apply_lda(z)\n",
    "        return z\n",
    "\n",
    "    def get_cv(self, x: Variable) -> (Variable):\n",
    "        return self.forward(x)\n",
    "\n",
    "# auxiliary class to export a model which outputs the topmost hidden layer\n",
    "class NN_Hidden(nn.Module):\n",
    "\n",
    "    def __init__(self, l ):\n",
    "        super(NN_Hidden, self).__init__()\n",
    "\n",
    "        modules=[]\n",
    "        for i in range( len(l)-1 ):\n",
    "            if( i<len(l)-2 ):\n",
    "                modules.append(nn.Linear(l[i], l[i+1]) )\n",
    "                modules.append( nn.ReLU(True) )\n",
    "            else:\n",
    "                modules.append(nn.Linear(l[i], l[i+1]) )\n",
    "\n",
    "        self.nn = nn.Sequential(*modules)\n",
    "\n",
    "        #norm option\n",
    "        self.normIn = False\n",
    "\n",
    "    def set_norm(self, Mean: torch.Tensor, Range: torch.Tensor):\n",
    "        self.normIn = True\n",
    "        self.Mean = Mean\n",
    "        self.Range = Range\n",
    "\n",
    "    def normalize(self, x: Variable):\n",
    "        batch_size = x.size(0)\n",
    "        x_size = x.size(1)\n",
    "        Mean = self.Mean.unsqueeze(0).expand(batch_size, x_size)\n",
    "        Range = self.Range.unsqueeze(0).expand(batch_size, x_size)\n",
    "        return x.sub(Mean).div(Range)\n",
    "\n",
    "    def get_hidden(self, x: Variable, svd=False, svd_vectors=False, svd_eigen=False, training=False) -> (Variable):\n",
    "        if(self.normIn):\n",
    "            x = self.normalize(x)\n",
    "        z = self.nn(x)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x: Variable) -> (Variable):\n",
    "        z = self.get_hidden(x,svd=False)\n",
    "        return z\n",
    "\n",
    "##################################\n",
    "# Loss function\n",
    "##################################\n",
    "\n",
    "def LDAloss_cholesky(H, label, test_routines=False):\n",
    "    #sizes\n",
    "    N, d = H.shape\n",
    "\n",
    "    # Mean centered observations for entire population\n",
    "    H_bar = H - torch.mean(H, 0, True)\n",
    "    #Total scatter matrix (cov matrix over all observations)\n",
    "    S_t = H_bar.t().matmul(H_bar) / (N - 1)\n",
    "    #Define within scatter matrix and compute it\n",
    "    S_w = torch.Tensor().new_zeros((d, d), device = device, dtype = dtype)\n",
    "    S_w_inv = torch.Tensor().new_zeros((d, d), device = device, dtype = dtype)\n",
    "    buf = torch.Tensor().new_zeros((d, d), device = device, dtype = dtype)\n",
    "    #Loop over classes to compute means and covs\n",
    "    for i in range(categ):\n",
    "        #check which elements belong to class i\n",
    "        H_i = H[torch.nonzero(label == i).view(-1)]\n",
    "        # compute mean centered obs of class i\n",
    "        H_i_bar = H_i - torch.mean(H_i, 0, True)\n",
    "        # count number of elements\n",
    "        N_i = H_i.shape[0]\n",
    "        if N_i == 0:\n",
    "            continue\n",
    "        S_w += H_i_bar.t().matmul(H_i_bar) / ((N_i - 1) * categ)\n",
    "\n",
    "    S_b = S_t - S_w\n",
    "\n",
    "    S_w = S_w + lambdA * torch.diag(torch.Tensor().new_ones((d), device = device, dtype = dtype))\n",
    "\n",
    "    ## Generalized eigenvalue problem: S_b * v_i = lambda_i * Sw * v_i\n",
    "\n",
    "    # (1) use cholesky decomposition for S_w\n",
    "    L = torch.cholesky(S_w,upper=False)\n",
    "\n",
    "    # (2) define new matrix using cholesky decomposition and\n",
    "    L_t = torch.t(L)\n",
    "    L_ti = torch.inverse(L_t)\n",
    "    L_i = torch.inverse(L)\n",
    "    S_new = torch.matmul(torch.matmul(L_i,S_b),L_ti)\n",
    "\n",
    "    # (3) solve  S_new * w_i = lambda_i * w_i\n",
    "    eig_values, eig_vectors = torch.symeig(S_new,eigenvectors=True)\n",
    "    eig_vectors = eig_vectors.t()\n",
    "    # (4) sort eigenvalues and retrieve old eigenvector\n",
    "    #eig_values, ind = torch.sort(eig_values, 0, descending=True)\n",
    "    max_eig_vector = eig_vectors[-1]\n",
    "    max_eig_vector = torch.matmul(L_ti,max_eig_vector)\n",
    "    norm=max_eig_vector.pow(2).sum().sqrt()\n",
    "    max_eig_vector.div_(norm)\n",
    "\n",
    "    loss = - eig_values[-1]\n",
    "\n",
    "    return loss, eig_values, max_eig_vector, S_b, S_w\n",
    "\n",
    "# Evaluate LDA over all the training set\n",
    "def check_LDA_cholesky(loader, model):\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            X,y = data[0].float().to(device),data[1].long().to(device)\n",
    "            H  = model.get_hidden(X)\n",
    "            _, eig_values, eig_vector, _, _ = LDAloss_cholesky(H, y)\n",
    "    return eig_values, eig_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lMiMbzFfNnWM"
   },
   "outputs": [],
   "source": [
    "#@title Encoding and plotting functions\n",
    "\n",
    "##################################\n",
    "# Encoding functions\n",
    "##################################\n",
    "\n",
    "def encode_hidden(loader,model,batch,n_hidden,device):\n",
    "    \"\"\"Compute the hidden layer over a dataloader\"\"\"\n",
    "    s=np.empty((len(loader),batch,n_hidden))\n",
    "    l=np.empty((len(loader),batch))\n",
    "    for i,data in enumerate(loader):\n",
    "        x,lab = data[0].float(),data[1].long()\n",
    "        x = Variable(x).to(device)\n",
    "        cv = model.get_hidden(x,svd=False)\n",
    "        #cv = model.apply_pca(cv)\n",
    "        s[i] = cv.detach().cpu().numpy()\n",
    "        l[i] = lab\n",
    "\n",
    "    s=s.reshape(len(loader)*batch,n_hidden)\n",
    "    s=s[0:len(loader)*batch]\n",
    "\n",
    "    l=l.reshape(len(loader)*batch)\n",
    "    l=l[0:len(loader)*batch]\n",
    "\n",
    "    sA = s[l==0]\n",
    "    sB = s[l==1]\n",
    "\n",
    "    return sA,sB\n",
    "\n",
    "def encode_cv(loader,model,batch,n_cv,device):\n",
    "    \"\"\"Compute the CV over a dataloader\"\"\"\n",
    "    s=np.empty((len(loader),batch,n_cv))\n",
    "    l=np.empty((len(loader),batch))\n",
    "    for i,data in enumerate(loader):\n",
    "        x,lab = data[0].float(),data[1].long()\n",
    "        x = Variable(x).to(device)\n",
    "        cv = model(x)\n",
    "        s[i] = cv.detach().cpu().numpy()\n",
    "        l[i] = lab\n",
    "\n",
    "    s=s.reshape(len(loader)*batch,n_cv)\n",
    "    s=s[0:len(loader)*batch]\n",
    "\n",
    "    l=l.reshape(len(loader)*batch)\n",
    "    l=l[0:len(loader)*batch]\n",
    "\n",
    "    sA = s[l==0]\n",
    "    sB = s[l==1]\n",
    "\n",
    "    return sA,sB\n",
    "\n",
    "def encode_cv_all(loader,model,batch,n_cv,device):\n",
    "    \"\"\"Compute the CV over a dataloader with labels\"\"\"\n",
    "    s=np.empty((len(loader),batch,n_cv))\n",
    "    l=np.empty((len(loader),batch))\n",
    "    for i,data in enumerate(loader):\n",
    "        x,lab = data[0].float(),data[1].long()\n",
    "        x = Variable(x).to(device)\n",
    "        cv = model.get_cv(x)\n",
    "        s[i] = cv.detach().cpu().numpy()\n",
    "        l[i] = lab\n",
    "\n",
    "    s=s.reshape(len(loader)*batch,n_cv)\n",
    "    s=s[0:len(loader)*batch]\n",
    "\n",
    "    l=l.reshape(len(loader)*batch)\n",
    "    l=l[0:len(loader)*batch]\n",
    "\n",
    "    return s,l\n",
    "\n",
    "##################################\n",
    "# Plotting functions\n",
    "##################################\n",
    "\n",
    "def plot_results(save=False,testing=False):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(13,4))\n",
    "    plot_training(axes[0],save)\n",
    "    plot_H(axes[1],save,testing)\n",
    "    plot_CV(axes[2],save,testing)\n",
    "    if save:\n",
    "        fig.savefig(\"{}/{}.png\".format(tr_folder, \"training\"),dpi=150)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_training(ax,save=False,training=False):\n",
    "    ax.set_title(\"Deep-LDA optimization\")\n",
    "    ax.plot(np.asarray(ep),np.asarray(eig),'.-', c='tab:green', label='train-batch')\n",
    "    ax.plot(np.asarray(ep),np.asarray(eig_t),'.-', c='tab:grey', label='train')\n",
    "    ax.plot(np.asarray(ep),np.asarray(eig_val),'.-', c='tab:orange', label='valid')\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"1st Eigenvalue\")\n",
    "    ax.legend()\n",
    "\n",
    "def plot_H(ax,save=False,testing=False):\n",
    "    ax.set_title(\"LDA on Hidden-space H\")\n",
    "    # -- Testing and Validation histograms --\n",
    "    trA,trB = encode_hidden(train_all_loader,model,train_data,n_hidden,device)\n",
    "    eigen=max_eig_vector.detach().numpy()\n",
    "\n",
    "    ax.scatter(trA[:,0],trA[:,1], c='tab:red', label='train A',alpha=0.3)\n",
    "    ax.scatter(trB[:,0],trB[:,1], c='tab:blue', label='train B',alpha=0.3)\n",
    "\n",
    "    if testing:\n",
    "        ttA,ttB = encode_hidden(valid_loader,model,valid_data,n_hidden,device)\n",
    "        ax.scatter(ttA[:,0],ttA[:,1], c='tab:orange', label='valid A',s=0.2, alpha=0.5)\n",
    "        ax.scatter(ttB[:,0],ttB[:,1], c='tab:cyan', label='valid B',s=0.2, alpha=0.5)\n",
    "        mIN=np.min([np.min(trA[:,0]),np.min(trB[:,0]),np.min(ttA[:,0]),np.min(ttB[:,0])])\n",
    "        mAX=np.max([np.max(trA[:,0]),np.max(trB[:,0]),np.max(ttA[:,0]),np.max(ttB[:,0])])\n",
    "    else:\n",
    "        mIN=np.min([np.min(trA[:,0]),np.min(trB[:,0])])\n",
    "        mAX=np.max([np.max(trA[:,0]),np.max(trB[:,0])])\n",
    "\n",
    "    ax.set_xlabel(r\"$h_0$\")\n",
    "    ax.set_ylabel(r\"$h_1$\")\n",
    "\n",
    "    #x=np.linspace(mIN,mAX,100)\n",
    "    #y=-eigen[0]/eigen[1]*x+0\n",
    "    #plt.plot(x,y, linewidth=2, label='DeepLDA')\n",
    "    ax.legend()\n",
    "\n",
    "def plot_CV(ax,save=False,testing=False):\n",
    "    sA,sB = encode_cv(train_all_loader,model,train_data,n_cv,device)\n",
    "    sA,sB = sA[:,0], sB[:,0]\n",
    "    if testing:\n",
    "        stA,stB = encode_cv(valid_loader,model,valid_data,n_cv,device)\n",
    "        stA,stB = stA[:,0], stB[:,0]\n",
    "        min_s=np.min([np.min(sA),np.min(sB),np.min(stA),np.min(stB)])\n",
    "        max_s=np.max([np.max(sA),np.max(sB),np.max(stA),np.max(stB)])\n",
    "    else:\n",
    "        min_s=np.min([np.min(sA),np.min(sB)])\n",
    "        max_s=np.max([np.max(sA),np.max(sB)])\n",
    "\n",
    "    b=np.linspace(min_s,max_s,100)\n",
    "\n",
    "    ax.set_title(\"Deep-LDA CV Histogram\")\n",
    "    ax.hist(sA, bins=b, ls='dashed', alpha = 0.7, lw=2, color='tab:red', label='train A',density=True)\n",
    "    ax.hist(sB, bins=b, ls='dashed', alpha = 0.7, lw=2, color='tab:blue', label='train B',density=True)\n",
    "\n",
    "    if testing:\n",
    "        plt.hist(stA, bins=b, ls='dashed', alpha = 0.5, lw=2, color='tab:orange', label='valid A',density=True)\n",
    "        plt.hist(stB, bins=b, ls='dashed', alpha = 0.5, lw=2, color='tab:cyan', label='valid B',density=True)\n",
    "\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGC0sg7KBcaZ"
   },
   "source": [
    "## **Load files**\n",
    "\n",
    "Specify dataset structure (use **from_column=1** to exclude time from COLVAR file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pn8UTmn8Re24",
    "outputId": "fe2acaa9-7cf3-4436-e854-bfdd79919691"
   },
   "outputs": [],
   "source": [
    "##@title **Load files**\n",
    "n_input =  17#no of descriptors {type:\"integer\"}\n",
    "from_column =  2#normally ignore first column, which is time {type:\"integer\"}\n",
    "\n",
    "#load files:\n",
    "#A.dat contains the information of state A; distA.shape= no_of_frames_in_A*17 \n",
    "#B.dat contains the information of state B; distB.shape= no_of_frames_in_B*17 \n",
    "\n",
    "distA = np.loadtxt('A.dat',usecols=range(from_column,from_column+n_input))\n",
    "distB = np.loadtxt('B.dat',usecols=range(from_column,from_column+n_input))\n",
    "\n",
    "print(\"[Imported data]\")\n",
    "print(\"- dataA.shape:\", distA.shape)\n",
    "print(\"- dataB.shape:\", distB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2hYdnYJamOr",
    "outputId": "cee2c48f-de8a-4202-e073-d1cf48c6bfcb"
   },
   "outputs": [],
   "source": [
    "##@title Create datasets\n",
    "standardize_inputs = True #@param {type:\"boolean\"}\n",
    "\n",
    "if standardize_inputs:\n",
    "    print(\"[Standardize inputs]\")\n",
    "    print(\"- Calculating mean and range over the training set\")\n",
    "    Max=np.amax(np.concatenate([distA,distB],axis=0),axis=0)\n",
    "    Min=np.amin(np.concatenate([distA,distB],axis=0),axis=0)\n",
    "\n",
    "    Mean=(Max+Min)/2.\n",
    "    Range=(Max-Min)/2.\n",
    "    if(np.sum(np.argwhere(Range<1e-6))>0):\n",
    "        print(\"- [Warning] Skipping normalization where range of values is < 1e-6. Input(s):\", np.argwhere(Range<1e-6).reshape(-1))\n",
    "        Range[Range<1e-6]=1.\n",
    "\n",
    "# create labels\n",
    "lA=np.zeros_like(distA[:,0])\n",
    "lB=np.ones_like(distB[:,0])\n",
    "\n",
    "dist=np.concatenate([distA,distB],axis=0)\n",
    "dist_label=np.concatenate([lA,lB],axis=0)\n",
    "\n",
    "print(dist.shape)\n",
    "\n",
    "p = np.random.permutation(len(dist))\n",
    "dist, dist_label = dist[p], dist_label[p]\n",
    "\n",
    "#@title Training and validation set size\n",
    "train_data = 12000#@param {type:\"integer\"}\n",
    "batch_tr = 4000#@param {type:\"integer\"}\n",
    "train_labels=ColvarDataset([dist[:train_data],dist_label[:train_data]])\n",
    "train_loader=DataLoader(train_labels, batch_size=batch_tr,shuffle=True)\n",
    "\n",
    "# create additional dataset which cover all the training set in one batch\n",
    "train_all_labels=ColvarDataset([dist[:train_data],dist_label[:train_data]])\n",
    "train_all_loader=DataLoader(train_all_labels, batch_size=train_data)\n",
    "\n",
    "#print(list(train_all_labels))\n",
    "\n",
    "# The validation is evaluated in a single batch\n",
    "valid_data = 4000#@param {type:\"integer\"}\n",
    "batch_val=valid_data\n",
    "valid_labels=ColvarDataset([dist[train_data:train_data+valid_data],dist_label[train_data:train_data+valid_data]])\n",
    "valid_loader=DataLoader(valid_labels, batch_size=batch_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IfmHhFZ3sjuq",
    "outputId": "ec0b37dc-f5b0-434f-e11b-5ffbf7c8c17d"
   },
   "outputs": [],
   "source": [
    "##@title NN and training parameters\n",
    "#type\n",
    "dtype = torch.float32\n",
    "# wheter to use CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "categ = 2\n",
    "n_cv=1\n",
    "\n",
    "hidden_nodes = \"20,20,5\" #@param {type:\"raw\"}\n",
    "nodes = [int(x)for x in hidden_nodes.split(',')]\n",
    "nodes.insert(0, n_input)\n",
    "n_hidden=nodes[-1]\n",
    "\n",
    "print(\"[NN Architecture]\")\n",
    "print(\"- hidden layers:\", nodes)\n",
    "print(\"\")\n",
    "print(\"========= NN =========\")\n",
    "model = NN_DeepLDA(nodes)\n",
    "if standardize_inputs:\n",
    "    model.set_norm(torch.tensor(Mean,dtype=dtype,device=device),torch.tensor(Range,dtype=dtype,device=device))\n",
    "print(\"======================\")\n",
    "model.to(device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"using CUDA acceleration\")\n",
    "    print(\"========================\")\n",
    "\n",
    "# -- Optimization --\n",
    "lrate = 0.001 #@param {type:\"slider\", min:0.0001, max:0.005, step:0.0001}\n",
    "lambdA = 0.05 #@param {type:\"number\"}\n",
    "l2_reg = 1e-5 #@param {type:\"number\"}\n",
    "act_reg = 1./lambdA # lorentzian regularization\n",
    "\n",
    "print(\"\")\n",
    "print(\"[Optimization]\")\n",
    "print(\"- Learning rate \\t=\",lrate)\n",
    "print(\"- l2 regularization \\t=\",l2_reg)\n",
    "print(\"- lambda (S_w reg.) \\t=\",lambdA)\n",
    "print(\"- lorentian (CV reg.) \\t=\",act_reg)\n",
    "\n",
    "#OPTIMIZERS\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lrate, weight_decay=l2_reg)\n",
    "\n",
    "#define arrays and values\n",
    "ep = []\n",
    "eig = []\n",
    "eig_t = []\n",
    "eig_val = []\n",
    "init_epoch = 0\n",
    "best_result = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZO9kC1ftuWGR",
    "outputId": "46327830-9274-4073-ddef-96ea88437466"
   },
   "outputs": [],
   "source": [
    "##@title Training\n",
    "num_epochs =  100#@param {type:\"number\"}\n",
    "print_loss = 1#@param {type:\"slider\", min:1, max:100, step:1}\n",
    "plot_every = 5#@param {type:\"slider\", min:1, max:100, step:1}\n",
    "plot_validation = True #@param {type:\"boolean\"}\n",
    "\n",
    "#format output\n",
    "float_formatter = lambda x: \"%.6f\" % x\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "\n",
    "print('[{:>3}/{:>3}] {:>10} {:>10} {:>10} {:>10}'.format('ep','tot','eig_train','eig_valid','reg loss','eigenvector'))\n",
    "\n",
    "# -- Training --\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        # =================get data===================\n",
    "        X,y = data[0].float().to(device),data[1].long().to(device)\n",
    "        # =================forward====================\n",
    "        #H,S = model.get_hidden(X,svd=True,svd_eigen=True)\n",
    "        H = model.get_hidden(X)\n",
    "        # =================lda loss===================\n",
    "        lossg, eig_values, max_eig_vector, Sb, Sw = LDAloss_cholesky(H, y)\n",
    "        model.set_lda(max_eig_vector)\n",
    "        s = model.apply_lda(H)\n",
    "        # =================reg loss===================\n",
    "        #reg_loss = H.pow(2).sum().div( H.size(0) )\n",
    "        #reg_loss_lor = - act_reg / (1+(reg_loss-1).pow(2))\n",
    "        Ha = H[y==0]\n",
    "        Hb = H[y==1]\n",
    "        reg_loss = Ha.pow(2).sum().div( Ha.size(0) ) + Hb.pow(2).sum().div( Hb.size(0) )\n",
    "        reg_loss_lor = - act_reg / (1+(reg_loss-1.0).pow(2))\n",
    "        # =================backprop===================\n",
    "        opt.zero_grad()\n",
    "        lossg.backward(retain_graph=True)\n",
    "        reg_loss_lor.backward()\n",
    "        opt.step()\n",
    "\n",
    "    #Compute LDA over entire datasets and save LDA eigenvector\n",
    "    train_eig_values, train_eig_vector = check_LDA_cholesky(train_all_loader, model)\n",
    "    model.set_lda(train_eig_vector)\n",
    "    valid_eig_values, valid_eig_vector = check_LDA_cholesky(valid_loader, model)\n",
    "\n",
    "    #save results\n",
    "    ep.append(epoch+init_epoch+1)\n",
    "    eig.append(eig_values.detach().numpy()[-1])\n",
    "    eig_t.append(train_eig_values[-1])\n",
    "    eig_val.append(valid_eig_values[-1])\n",
    "\n",
    "    if (epoch+1)%print_loss == 0:\n",
    "        print('[{:3d}/{:3d}] {:10.3f} {:10.3f} {:10.3f} '.format\n",
    "          (init_epoch+epoch+1, init_epoch+num_epochs, train_eig_values.detach().numpy()[-1], valid_eig_values.numpy()[-1], reg_loss), train_eig_vector.numpy() )\n",
    "\n",
    "    if (epoch+1)%plot_every == 0:\n",
    "        plot_results(testing=plot_validation)\n",
    "\n",
    "init_epoch += num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "uIwfIXr1hAR3",
    "outputId": "4c4b2ea6-8437-4d38-dd18-ab767ddda87a"
   },
   "outputs": [],
   "source": [
    "#@title Plot hidden components\n",
    "#encode_hidden computes the hidden variables for an entire dataset, using the specified model\n",
    "\n",
    "trA,trB = encode_hidden(train_all_loader,model,train_data,n_hidden,device)\n",
    "print(np.shape(trA), np.shape(trB))\n",
    "fig, axs = plt.subplots(1, n_hidden, figsize=(3.5*n_hidden,3.5))\n",
    "fig.suptitle('Hidden components')\n",
    "for i in range(n_hidden):\n",
    "    ax = axs[i]\n",
    "    ax.set_xlabel(\"Training set\")\n",
    "    ax.set_ylabel(r\"$h_\"+str(i)+\"$\")\n",
    "    ax.plot(trA[:,i], c='tab:red', label='trA')\n",
    "    ax.plot(trB[:,i], c='tab:blue', label='trB')\n",
    "    ax.legend()\n",
    "\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "LRsMCzgloYI1",
    "outputId": "3a18a5cd-f233-459e-f216-a48e85f2c763"
   },
   "outputs": [],
   "source": [
    "#@title Scatter plot w/LDA boundaries\n",
    "from itertools import combinations\n",
    "\n",
    "#get LDA eigenvector\n",
    "eigen = model.get_lda().numpy()[0]\n",
    "\n",
    "#compute h\n",
    "trA,trB = encode_hidden(train_all_loader,model,train_data,n_hidden,device)\n",
    "ttA,ttB = encode_hidden(valid_loader,model,valid_data,n_hidden,device)\n",
    "\n",
    "#plot layout\n",
    "n_plots=len(list(combinations(range(n_hidden),2)))\n",
    "num_plots_per_line = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "num_lines=int(n_plots/num_plots_per_line+0.999)\n",
    "\n",
    "#define subplots\n",
    "fig, axs = plt.subplots(num_lines, num_plots_per_line, figsize=(3*num_plots_per_line,3*num_lines),dpi=100)\n",
    "axs = axs.reshape(-1)\n",
    "fig.suptitle('Scatter plots of hidden components')\n",
    "\n",
    "#iterate and plot\n",
    "idx=0\n",
    "point_size=2 #@param {type:\"slider\", min:0.5, max:20, step:0.5}\n",
    "for i,j in combinations(range(n_hidden),2):\n",
    "    ax = axs[idx]\n",
    "    idx+=1\n",
    "    ax.scatter(trA[:,i],trA[:,j], c='tab:red', s=point_size, label='trA')\n",
    "    ax.scatter(ttA[:,i],ttA[:,j], c='tab:orange', s=point_size, label='testA',alpha=0.3,marker='o')\n",
    "    ax.scatter(trB[:,i],trB[:,j], c='tab:blue', s=point_size, label='trB')\n",
    "    ax.scatter(ttB[:,i],ttB[:,j], c='tab:cyan', s=point_size, label='testB',alpha=0.3,marker='o')\n",
    "    #plot LDA line\n",
    "    mIN=np.min([np.min(trA[:,i]),np.min(trB[:,i]),np.min(ttA[:,i]),np.min(ttB[:,i])])\n",
    "    mAX=np.max([np.max(trA[:,i]),np.max(trB[:,i]),np.max(ttA[:,i]),np.max(ttB[:,i])])\n",
    "    x=np.linspace(mIN,mAX,100)\n",
    "    y=-eigen[i]/eigen[j]*x+0\n",
    "    ax.plot(x,y, linewidth=2,  label='LDA boundary', color='darkgrey',alpha=0.7,linestyle='dashed')\n",
    "    #labels\n",
    "    ax.set_xlabel(r\"$h_\"+str(i)+\"$\")\n",
    "    ax.set_ylabel(r\"$h_\"+str(j)+\"$\")\n",
    "    if idx == 1:\n",
    "        leg=ax.legend(loc='lower left', bbox_to_anchor= (0.0, 1.01), ncol=5,\n",
    "            borderaxespad=0, frameon=False)\n",
    "        leg.set_in_layout(False)\n",
    "\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.93])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_distA = torch.tensor(distA, dtype=torch.float)\n",
    "torch_distB = torch.tensor(distB, dtype=torch.float)\n",
    "\n",
    "resultA = model(torch_distA)\n",
    "resultB = model(torch_distB)\n",
    "np.savetxt(\"A_from_model.dat\", resultA.detach().numpy())\n",
    "np.savetxt(\"B_from_model.dat\", resultB.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JW7sZ9ak8av"
   },
   "source": [
    "## **Download model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "jVvQPx6leq8Z",
    "outputId": "e927a61d-c86a-4b35-d293-f8b764c5776d"
   },
   "outputs": [],
   "source": [
    "##@title Download model\n",
    "\n",
    "model_name = \"model_walp\" #@param {type:\"string\"}\n",
    "save_hidden_layer_model = True #@param {type:\"boolean\"}\n",
    "save_lda_coeffs = True #@param {type:\"boolean\"}\n",
    "save_pictures = True #@param {type:\"boolean\"}\n",
    "save_checkpoint = True #@param {type:\"boolean\"}\n",
    "\n",
    "# == Set output folder\n",
    "tr_folder=model_name+\"/\"\n",
    "!mkdir -p \"{tr_folder}\"\n",
    "\n",
    "# == Create fake dataloader ==\n",
    "fake_loader = DataLoader(train_labels, batch_size=1,shuffle=False)\n",
    "fake_input = next(iter(fake_loader ))[0].float()\n",
    "\n",
    "# == Export model ==\n",
    "mod = torch.jit.trace(model, fake_input)\n",
    "mod.save(tr_folder+model_name+\".pt\")\n",
    "\n",
    "if save_hidden_layer_model:\n",
    "    model2 = NN_Hidden(nodes)\n",
    "    if standardize_inputs:\n",
    "        model2.set_norm(torch.tensor(Mean,dtype=dtype,device=device),torch.tensor(Range,dtype=dtype,device=device))\n",
    "    #transfer parameters\n",
    "    params = model.named_parameters()\n",
    "    params2 = model2.named_parameters()\n",
    "    dict_params2 = dict(params2)\n",
    "    for name, param in params:\n",
    "        if name in dict_params2:\n",
    "            dict_params2[name].data.copy_(param.data)\n",
    "    # save model\n",
    "    mod2 = torch.jit.trace(model2, fake_input)\n",
    "    mod2.save(tr_folder+model_name+\"_hidden.pt\")\n",
    "\n",
    "if save_lda_coeffs:\n",
    "    f = open(tr_folder+\"lda.dat\", \"w\")\n",
    "    f.write(str(model.get_lda().numpy()))\n",
    "    f.close()\n",
    "    \n",
    "if save_pictures:\n",
    "    plot_results(save=True,testing=True)\n",
    "\n",
    "if save_checkpoint:\n",
    "    torch.save(model.state_dict(), tr_folder+model_name+'_state_dict.pt')\n",
    "    np.savetxt(tr_folder+model_name+'_mean.dat', Mean)\n",
    "    np.savetxt(tr_folder+model_name+'_range.dat', Range)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hpBg5KGjqp6-",
    "gc5aS9mXCqqc",
    "J8Z_5iTY_nTm",
    "FbPMFpDSCFsz",
    "m18wvZQg_u0x",
    "OdE4CWlAS1Yr",
    "rlLw1y9R0mJh",
    "QLa3U64Gyr5R",
    "KpYwhoLkGzdU",
    "5pT8FPdsGzd2",
    "VqoqUEpxAgHY",
    "PrxXGowLGzeA",
    "iSvAhLdZeHBf",
    "U_8bgedTeKcM",
    "GxB5HqczeKcP",
    "J-YznZt_0Hqf",
    "QzGbYHX-0Hqj",
    "Ud5SwKYy0Hqs",
    "j--leyHs0Hq0",
    "yLEWcR0u0Hq-",
    "MvkeMKmB0HrB",
    "BPy0JOH90HrG",
    "YGPqaELn0HrJ",
    "uIYzU-jS0HrM",
    "KRbbqfic8yTq",
    "dT4ZswlG8yUC",
    "Bhh6w75akGRb",
    "el8tiGw_Rs3-",
    "2Rcb2IusAgRi",
    "UZFOf5cv8Om6",
    "iyooOzpf7JiM",
    "h0oCzn4v8yUM",
    "LjF9hUvI4ESz",
    "6PatlEDd8yUQ",
    "1sOOL5upZG0r",
    "Egw88T9tk-EV",
    "4Y5POrHtddwa",
    "iekpwjhOZG1C",
    "c7xjE7cgfuJS",
    "UT51npndD1mz",
    "Li-UQ5wu8yUU",
    "QAbwFHhJ8yUY",
    "eRIkatma8yUc",
    "qrvTAYob8yUg",
    "J1WWc3Hj8yUi",
    "lRk0l699yxhU",
    "dIUsSjCTFFos",
    "0XWHE2Yyj6Ng",
    "ZKXo6EL-rLRX",
    "OWgHfU1sLAQp",
    "JsLqRecHGpKc",
    "XjU8FW7AW1t0",
    "Dwav6dtBAYHx",
    "kYigkC1thrAZ",
    "e5uzJyJKhrAw",
    "0PegXdrXhrA4",
    "y6Ox46eTfDfK",
    "tD9AnFmfhrA-",
    "87TLL5PghrBE",
    "4bjo8CuxhrBI",
    "gyrFnobgaCvC",
    "_5iGmT4Prj3q",
    "sZjdSmc2_cBl",
    "R95FsTP1GW7l",
    "bpcLcfgm_eEQ",
    "sGb10DCR_iwW",
    "vS5UW8sD_kQt",
    "NL28tJ1_6mal",
    "Jnygy_dQrtPE",
    "QkgmTCcBCiEb",
    "5E-2s2DZob0z",
    "JZXnDZzmCZBw",
    "CbOmtoIyCaMq",
    "Z0GwfSo5ITyd",
    "98IadJTJUKBJ",
    "6b0l2nm_pvhJ",
    "NQWuGCwcrtPz",
    "S0JjX2KPEMtB",
    "uvWDvkd-A9Sl",
    "xAJKmezu_Q1a",
    "6xFJxUcaBdbV",
    "Tn-qeP57pnUf",
    "59aDvx2QEUGO",
    "mYlIuE-aAnKa",
    "oYox1YFHzi5c",
    "BSBsU77EaeLY",
    "xxy7lr90anBP",
    "cU4zTG3-bxMH",
    "JERty12gb6E8",
    "P3UqWHr_tuDY",
    "SDI5lFm-tuDc",
    "BvAXa4-htuDq",
    "gl7nSfMrtuD4",
    "orC7IlRLtuD-",
    "ZmG_8VCp8zjO",
    "-qnu0E3stuEC",
    "pr0LCGlFTE8u",
    "HZArjFSLTE80",
    "cl04e0LkTE82",
    "t9_aT3d8TE85",
    "I_zwriisTE88",
    "yDkerjTdqEEy",
    "ja4RFVzdqEE1",
    "tj3oOJ2uQ0m6",
    "jFbM757CQ0m_",
    "3Jm0QF2B7oEs",
    "rpzHvlPGeWv2",
    "hXmUqJ1GpZFe",
    "38cFqN0ppZFf",
    "x-bV9_w9pZFm",
    "sbwzgjLuloc4",
    "1inmmw-bpZFs",
    "46_Pi7GTpZF5",
    "EY-ftzc6B6Mv",
    "ojeiK0Jg9rkR",
    "9BfjANwcvRQ8",
    "ds6D1xxxvVDo",
    "MPuXelc9T_O1",
    "VwzP0yVNzsMt",
    "ZV4sT-rXCEd4",
    "myDIH5ERnc0l",
    "epZDhmVDe1Zm",
    "m_o-hTij1sYt"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
