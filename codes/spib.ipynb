{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import matplotlib\n",
    "from matplotlib import colors as c\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_init(traj_label, past_traj_data, future_traj_data, traj_weights):\n",
    "    assert len(future_traj_data)==len(traj_label)\n",
    "    \n",
    "    # skip the first t0 data\n",
    "    past_data = past_traj_data\n",
    "    future_data = future_traj_data\n",
    "    label = traj_label\n",
    "    \n",
    "    # data shape\n",
    "    data_shape = past_data.shape[1:]\n",
    "    \n",
    "    n_data = len(past_data)\n",
    "    \n",
    "    # 90% random test/train split\n",
    "    p = np.random.permutation(n_data)\n",
    "    past_data = past_data[p]\n",
    "    future_data = future_data[p]\n",
    "    label = label[p]\n",
    "    \n",
    "    past_data_train = past_data[0: (9 * n_data) // 10]\n",
    "    past_data_test = past_data[(9 * n_data) // 10:]\n",
    "    \n",
    "    future_data_train = future_data[0: (9 * n_data) // 10]\n",
    "    future_data_test = future_data[(9 * n_data) // 10:]\n",
    "    \n",
    "    label_train = label[0: (9 * n_data) // 10]\n",
    "    label_test = label[(9 * n_data) // 10:]\n",
    "    \n",
    "    if traj_weights != None:\n",
    "        assert len(traj_data)==len(traj_weights)\n",
    "        weights = traj_weights[t0:(len(traj_data)-dt)]\n",
    "        weights = weights[p]\n",
    "        weights_train = weights[0: (9 * n_data) // 10]\n",
    "        weights_test = weights[(9 * n_data) // 10:]\n",
    "    else:\n",
    "        weights_train = None\n",
    "        weights_test = None\n",
    "    \n",
    "    return data_shape, past_data_train, future_data_train, label_train, weights_train,\\\n",
    "        past_data_test, future_data_test, label_test, weights_test\n",
    "\n",
    "# Loss function\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def calculate_loss(IB, data_inputs, data_targets, data_weights, beta=1.0):\n",
    "    \n",
    "    # pass through VAE\n",
    "    outputs, z_sample, z_mean, z_logvar = IB.forward(data_inputs)\n",
    "    \n",
    "    # KL Divergence\n",
    "    log_p = IB.log_p(z_sample)\n",
    "    log_q = -0.5 * torch.sum(z_logvar + torch.pow(z_sample-z_mean, 2)\n",
    "                             /torch.exp(z_logvar), dim=1)\n",
    "    \n",
    "    if data_weights == None:\n",
    "        # Reconstruction loss is cross-entropy\n",
    "        reconstruction_error = torch.mean(torch.sum(-data_targets*outputs, dim=1))\n",
    "        \n",
    "        # KL Divergence\n",
    "        kl_loss = torch.mean(log_q-log_p)\n",
    "        \n",
    "    else:\n",
    "        # Reconstruction loss is cross-entropy\n",
    "        # reweighed\n",
    "        reconstruction_error = torch.mean(data_weights*torch.sum(-data_targets*outputs, dim=1))\n",
    "        \n",
    "        # KL Divergence\n",
    "        kl_loss = torch.mean(data_weights*(log_q-log_p))\n",
    "        \n",
    "    \n",
    "    loss = reconstruction_error + beta*kl_loss\n",
    "\n",
    "    return loss, reconstruction_error.float(), kl_loss.float()\n",
    "\n",
    "\n",
    "# Train and test model\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def sample_minibatch(past_data, data_labels, data_weights, indices, device):\n",
    "    sample_past_data = past_data[indices].to(device)\n",
    "    sample_data_labels = data_labels[indices].to(device)\n",
    "    \n",
    "    if data_weights == None:\n",
    "        sample_data_weights = None\n",
    "    else:\n",
    "        sample_data_weights = data_weights[indices].to(device)\n",
    "    \n",
    "    \n",
    "    return sample_past_data, sample_data_labels, sample_data_weights\n",
    "\n",
    "\n",
    "def train(IB, beta, train_past_data, train_future_data, init_train_data_labels, train_data_weights, \\\n",
    "          test_past_data, test_future_data, init_test_data_labels, test_data_weights, \\\n",
    "              learning_rate, lr_scheduler_step_size, lr_scheduler_gamma, batch_size, threshold, patience, refinements, log_interval, device, index):\n",
    "    IB.train()\n",
    "    \n",
    "    step = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    train_data_labels = init_train_data_labels\n",
    "    test_data_labels = init_test_data_labels\n",
    "\n",
    "    update_times = 0\n",
    "    unchanged_epochs = 0\n",
    "    epoch = 0\n",
    "\n",
    "    # initial state population\n",
    "    state_population0 = torch.sum(train_data_labels,dim=0).float()/train_data_labels.shape[0]\n",
    "\n",
    "    # generate the optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(IB.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_scheduler_step_size, gamma=lr_scheduler_gamma)\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        train_permutation = torch.randperm(len(train_past_data))\n",
    "        test_permutation = torch.randperm(len(test_past_data))\n",
    "        \n",
    "        \n",
    "        for i in range(0, len(train_past_data), batch_size):\n",
    "            step += 1\n",
    "            \n",
    "            if i+batch_size>len(train_past_data):\n",
    "                break\n",
    "            \n",
    "            train_indices = train_permutation[i:i+batch_size]\n",
    "            \n",
    "            batch_inputs, batch_outputs, batch_weights = sample_minibatch(train_past_data, train_data_labels, \\\n",
    "                                                                       train_data_weights, train_indices, device)\n",
    "                    \n",
    "            loss, reconstruction_error, kl_loss= calculate_loss(IB, batch_inputs, \\\n",
    "                                                                batch_outputs, batch_weights, beta)\n",
    "            \n",
    "            # Stop if NaN is obtained\n",
    "            if(torch.isnan(loss).any()):\n",
    "                return True\n",
    "            \n",
    "            outputs, z_sample, z_mean, z_logvar = IB.forward(batch_inputs)\n",
    "            spa_loss = z_mean.pow(2).sum().div( z_mean.size(0) )\n",
    "            spa_loss_sig = 1/ (1+torch.exp(-(spa_loss-10)))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            spa_loss_sig.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if step % 500 == 0:\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    batch_inputs, batch_outputs, batch_weights = sample_minibatch(train_past_data, train_data_labels, \\\n",
    "                                                                               train_data_weights, train_indices, device)\n",
    "                            \n",
    "                    loss, reconstruction_error, kl_loss= calculate_loss(IB, batch_inputs, \\\n",
    "                                                                        batch_outputs, batch_weights, beta)\n",
    "                    train_time = time.time() - start\n",
    "                    \n",
    "                    outputs, z_sample, z_mean, z_logvar = IB.forward(batch_inputs)\n",
    "                    spa_loss = z_mean.pow(2).sum().div( z_mean.size(0) )\n",
    "                    spa_loss_sig = 1/ (1+torch.exp(-(spa_loss-10)))\n",
    "\n",
    "                    print(\"Iteration %i:\\tTime %f s\\nLoss (train) %f\\tKL loss (train): %f\\n\"\n",
    "                        \"Reconstruction loss (train) %f\\t Spatial loss %f\" % (\n",
    "                            step, train_time, loss, kl_loss, reconstruction_error,spa_loss_sig))\n",
    "                    \n",
    "                    j=i%len(test_permutation)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    test_indices = test_permutation[j:j+batch_size]\n",
    "                    \n",
    "                    batch_inputs, batch_outputs, batch_weights = sample_minibatch(test_past_data, test_data_labels, \\\n",
    "                                                                               test_data_weights, test_indices, device)\n",
    "                    \n",
    "                    loss, reconstruction_error, kl_loss = calculate_loss(IB, batch_inputs, \\\n",
    "                                                                         batch_outputs, batch_weights, beta)\n",
    "\n",
    "                    train_time = time.time() - start\n",
    "                    print(\"Loss (test) %f\\tKL loss (test): %f\\n\"\n",
    "                       \"Reconstruction loss (test) %f\" % (\n",
    "                           loss, kl_loss, reconstruction_error))\n",
    "                    \n",
    "\n",
    "        epoch+=1\n",
    "        \n",
    "        # check convergence\n",
    "        new_train_data_labels = IB.update_labels(train_future_data, batch_size)\n",
    "\n",
    "        # save the state population\n",
    "        state_population = torch.sum(new_train_data_labels,dim=0).float()/new_train_data_labels.shape[0]\n",
    "\n",
    "        print(state_population)\n",
    "\n",
    "        # print the state population change\n",
    "        state_population_change = torch.sqrt(torch.square(state_population-state_population0).sum())\n",
    "        \n",
    "        print('State population change=%f'%state_population_change)\n",
    "\n",
    "        # update state_population\n",
    "        state_population0 = state_population\n",
    "\n",
    "        scheduler.step()\n",
    "        if scheduler.gamma < 1:\n",
    "            print(\"Update lr to %f\"%(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "        # check whether the change of the state population is smaller than the threshold\n",
    "        if state_population_change < threshold:\n",
    "            unchanged_epochs += 1\n",
    "            \n",
    "            if unchanged_epochs > patience:\n",
    "\n",
    "                # check whether only one state is found\n",
    "                if torch.sum(state_population>0)<2:\n",
    "                    print(\"Only one metastable state is found!\")\n",
    "                    break\n",
    "\n",
    "                # Stop only if update_times >= refinements\n",
    "                if IB.UpdateLabel and update_times < refinements:\n",
    "                    \n",
    "                    train_data_labels = new_train_data_labels\n",
    "                    test_data_labels = IB.update_labels(test_future_data, batch_size)\n",
    "    \n",
    "                    update_times+=1\n",
    "                    print(\"Update %d\\n\"%(update_times))\n",
    "                    \n",
    "                    # reset epoch and unchanged_epochs\n",
    "                    epoch = 0\n",
    "                    unchanged_epochs = 0\n",
    "\n",
    "                    # reset the representative-inputs\n",
    "                    representative_inputs = IB.estimatate_representative_inputs(train_past_data, train_data_weights, batch_size)\n",
    "                    IB.reset_representative(representative_inputs.to(device))\n",
    "    \n",
    "                    # reset the optimizer and scheduler\n",
    "                    optimizer = torch.optim.Adam(IB.parameters(), lr=learning_rate)\n",
    "\n",
    "                    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_scheduler_step_size, gamma=lr_scheduler_gamma)\n",
    "                    \n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            unchanged_epochs = 0\n",
    "\n",
    "        print(\"Epoch: %d\\n\"%(epoch))\n",
    "\n",
    "    # output the saving path\n",
    "    total_training_time = time.time() - start\n",
    "    print(\"Total training time: %f\" % total_training_time)\n",
    "\n",
    "    return False\n",
    "\n",
    "def output_final_result(IB, device, train_past_data, train_future_data, train_data_labels, train_data_weights, \\\n",
    "                        test_past_data, test_future_data, test_data_labels, test_data_weights, batch_size, \\\n",
    "                             dt, beta, learning_rate, index=0):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # label update\n",
    "        if IB.UpdateLabel:\n",
    "            train_data_labels = IB.update_labels(train_future_data, batch_size)\n",
    "            test_data_labels = IB.update_labels(test_future_data, batch_size)\n",
    "        \n",
    "        final_result = []\n",
    "        # output the result\n",
    "        \n",
    "        loss, reconstruction_error, kl_loss= [0 for i in range(3)]\n",
    "        \n",
    "        for i in range(0, len(train_past_data), batch_size):\n",
    "            batch_inputs, batch_outputs, batch_weights = sample_minibatch(train_past_data, train_data_labels, train_data_weights, \\\n",
    "                                                                       range(i,min(i+batch_size,len(train_past_data))), IB.device)\n",
    "            loss1, reconstruction_error1, kl_loss1 = calculate_loss(IB, batch_inputs, batch_outputs, \\\n",
    "                                                                    batch_weights, beta)\n",
    "            loss += loss1*len(batch_inputs)\n",
    "            reconstruction_error += reconstruction_error1*len(batch_inputs)\n",
    "            kl_loss += kl_loss1*len(batch_inputs)\n",
    "            \n",
    "        \n",
    "        # output the result\n",
    "        loss/=len(train_past_data)\n",
    "        reconstruction_error/=len(train_past_data)\n",
    "        kl_loss/=len(train_past_data)\n",
    "                \n",
    "        final_result += [loss.data.cpu().numpy(), reconstruction_error.cpu().data.numpy(), kl_loss.cpu().data.numpy()]\n",
    "        print(\"Final: %d\\nLoss (train) %f\\tKL loss (train): %f\\n\"\n",
    "                    \"Reconstruction loss (train) %f\" % (index, loss, kl_loss, reconstruction_error))\n",
    "       \n",
    "    \n",
    "        loss, reconstruction_error, kl_loss = [0 for i in range(3)]\n",
    "        \n",
    "        for i in range(0, len(test_past_data), batch_size):\n",
    "            batch_inputs, batch_outputs, batch_weights = sample_minibatch(test_past_data, test_data_labels, test_data_weights, \\\n",
    "                                                                                         range(i,min(i+batch_size,len(test_past_data))), IB.device)\n",
    "            loss1, reconstruction_error1, kl_loss1 = calculate_loss(IB, batch_inputs, batch_outputs, \\\n",
    "                                                                   batch_weights, beta)\n",
    "            loss += loss1*len(batch_inputs)\n",
    "            reconstruction_error += reconstruction_error1*len(batch_inputs)\n",
    "            kl_loss += kl_loss1*len(batch_inputs)\n",
    "            \n",
    "        \n",
    "        # output the result\n",
    "        loss/=len(test_past_data)\n",
    "        reconstruction_error/=len(test_past_data)\n",
    "        kl_loss/=len(test_past_data)\n",
    "        \n",
    "        final_result += [loss.cpu().data.numpy(), reconstruction_error.cpu().data.numpy(), kl_loss.cpu().data.numpy()]\n",
    "        print(\"Loss (test) %f\\tKL loss (train): %f\\n\"\n",
    "            \"Reconstruction loss (test) %f\"% (loss, kl_loss, reconstruction_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPIB(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_type, z_dim, output_dim, data_shape, device, UpdateLabel= False, neuron_num1=128, \n",
    "                 neuron_num2=128):\n",
    "        \n",
    "        super(SPIB, self).__init__()\n",
    "        if encoder_type == 'Nonlinear':\n",
    "            self.encoder_type = 'Nonlinear'\n",
    "        else:\n",
    "            self.encoder_type = 'Linear'\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.neuron_num1 = neuron_num1\n",
    "        self.neuron_num2 = neuron_num2\n",
    "        \n",
    "        self.data_shape = data_shape\n",
    "        \n",
    "        self.UpdateLabel = UpdateLabel\n",
    "        \n",
    "        self.eps = 1e-10\n",
    "        self.device = device\n",
    "        \n",
    "        \n",
    "\n",
    "        # representative-inputs\n",
    "        self.representative_dim = output_dim\n",
    "\n",
    "        # torch buffer, these variables will not be trained\n",
    "        self.representative_inputs = torch.eye(self.output_dim, np.prod(self.data_shape), device=device, requires_grad=False)\n",
    "        \n",
    "        # create an idle input for calling representative-weights\n",
    "        # torch buffer, these variables will not be trained\n",
    "        self.idle_input = torch.eye(self.output_dim, self.output_dim, device=device, requires_grad=False)\n",
    "\n",
    "        # representative weights\n",
    "        self.representative_weights = nn.Sequential(\n",
    "            nn.Linear(self.output_dim, 1, bias=False),\n",
    "            nn.Softmax(dim=0))\n",
    "        \n",
    "        self.encoder = self._encoder_init()\n",
    "\n",
    "        if self.encoder_type == 'Nonlinear': \n",
    "            self.encoder_mean = nn.Linear(self.neuron_num1, self.z_dim)\n",
    "        else:\n",
    "            self.encoder_mean = nn.Linear(np.prod(self.data_shape), self.z_dim)\n",
    "        \n",
    "        # Note: encoder_type = 'Linear' only means that z_mean is a linear combination of the input OPs, \n",
    "        # the log_var is always obtained through a nonlinear NN\n",
    "\n",
    "        # enforce log_var in the range of [-10, 0]\n",
    "        self.encoder_logvar = nn.Sequential(\n",
    "            nn.Linear(self.neuron_num1, self.z_dim),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        self.decoder = self._decoder_init()\n",
    "        \n",
    "    def _encoder_init(self):\n",
    "        \n",
    "        modules = [nn.Linear(np.prod(self.data_shape), self.neuron_num1)]\n",
    "        modules += [nn.ReLU()]\n",
    "        for _ in range(1):\n",
    "            modules += [nn.Linear(self.neuron_num1, self.neuron_num1)]\n",
    "            modules += [nn.ReLU()]\n",
    "        \n",
    "        return nn.Sequential(*modules)\n",
    "    \n",
    "    def _decoder_init(self):\n",
    "        # cross-entropy MLP decoder\n",
    "        # output the probability of future state\n",
    "        modules = [nn.Linear(self.z_dim, self.neuron_num2)]\n",
    "        modules += [nn.ReLU()]\n",
    "        for _ in range(1):\n",
    "            modules += [nn.Linear(self.neuron_num2, self.neuron_num2)]\n",
    "            modules += [nn.ReLU()]\n",
    "        \n",
    "        modules += [nn.Linear(self.neuron_num2, self.output_dim)]\n",
    "        modules += [nn.LogSoftmax(dim=1)]\n",
    "        \n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "    \n",
    "    def encode(self, inputs):\n",
    "        enc = self.encoder(inputs)\n",
    "        \n",
    "        if self.encoder_type == 'Nonlinear': \n",
    "            z_mean = self.encoder_mean(enc)\n",
    "        else:\n",
    "            z_mean = self.encoder_mean(inputs)\n",
    "\n",
    "        # Note: encoder_type = 'Linear' only means that z_mean is a linear combination of the input OPs, \n",
    "        # the log_var is always obtained through a nonlinear NN\n",
    "        \n",
    "        # enforce log_var in the range of [-10, 0]\n",
    "        z_logvar = -10*self.encoder_logvar(enc)\n",
    "        \n",
    "        return z_mean, z_logvar\n",
    "    \n",
    "    def forward(self, data):\n",
    "        inputs = torch.flatten(data, start_dim=1)\n",
    "        \n",
    "        z_mean, z_logvar = self.encode(inputs)\n",
    "        \n",
    "        z_sample = self.reparameterize(z_mean, z_logvar)\n",
    "        \n",
    "        outputs = self.decoder(z_sample)\n",
    "        \n",
    "        return outputs, z_sample, z_mean, z_logvar\n",
    "    \n",
    "    def log_p (self, z, sum_up=True):\n",
    "        # get representative_z - representative_dim * z_dim\n",
    "        representative_z_mean, representative_z_logvar = self.get_representative_z()\n",
    "        # get representative weights - representative_dim * 1\n",
    "        w = self.representative_weights(self.idle_input)\n",
    "        # w = 0.5*torch.ones((2,1)).to(self.device)\n",
    "        \n",
    "        # expand z - batch_size * z_dim\n",
    "        z_expand = z.unsqueeze(1)\n",
    "        \n",
    "        representative_mean = representative_z_mean.unsqueeze(0)\n",
    "        representative_logvar = representative_z_logvar.unsqueeze(0)\n",
    "        \n",
    "        # representative log_q\n",
    "        representative_log_q = -0.5 * torch.sum(representative_logvar + torch.pow(z_expand-representative_mean, 2)\n",
    "                                        / torch.exp(representative_logvar), dim=2 )\n",
    "        \n",
    "        if sum_up:\n",
    "            log_p = torch.sum(torch.log(torch.exp(representative_log_q)@w + self.eps), dim=1)\n",
    "        else:\n",
    "            log_p = torch.log(torch.exp(representative_log_q)*w.T + self.eps)  \n",
    "            \n",
    "        return log_p\n",
    "        \n",
    "    # the prior\n",
    "    def get_representative_z(self):\n",
    "        # calculate representative_means\n",
    "        # with torch.no_grad():\n",
    "        X = self.representative_inputs\n",
    "\n",
    "        # calculate representative_z\n",
    "        representative_z_mean, representative_z_logvar = self.encode(X)  # C x M\n",
    "\n",
    "        return representative_z_mean, representative_z_logvar\n",
    "\n",
    "    def reset_representative(self, representative_inputs):\n",
    "        \n",
    "        # reset the nuber of representative inputs   \n",
    "        self.representative_dim = representative_inputs.shape[0]        \n",
    "        \n",
    "        # reset representative weights\n",
    "        self.idle_input = torch.eye(self.representative_dim, self.representative_dim, device=self.device, requires_grad=False)\n",
    "\n",
    "        self.representative_weights = nn.Sequential(\n",
    "            nn.Linear(self.representative_dim, 1, bias=False),\n",
    "            nn.Softmax(dim=0))\n",
    "        self.representative_weights[0].weight = nn.Parameter(torch.ones([1, self.representative_dim], device=self.device))\n",
    "        \n",
    "        # reset representative inputs\n",
    "        self.representative_inputs = representative_inputs.clone().detach()\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def init_representative_inputs(self, inputs, labels):\n",
    "        state_population = labels.sum(dim=0).cpu()\n",
    "        \n",
    "        # randomly pick up one sample from each initlal state as the initial guess of representative-inputs\n",
    "        representative_inputs=[]\n",
    "        \n",
    "        for i in range(state_population.shape[-1]):\n",
    "            if state_population[i]>0:\n",
    "                index = np.random.randint(0,state_population[i])\n",
    "                representative_inputs+=[inputs[labels[:,i].bool()][index].reshape(1,-1)]\n",
    "                # print(index)\n",
    "        \n",
    "        representative_inputs = torch.cat(representative_inputs, dim=0)\n",
    "\n",
    "        self.reset_representative(representative_inputs.to(self.device))\n",
    "            \n",
    "        return representative_inputs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimatate_representative_inputs(self, inputs, bias, batch_size):\n",
    "        prediction = []\n",
    "        mean_rep = []\n",
    "        for i in range(0, len(inputs), batch_size):\n",
    "            batch_inputs = inputs[i:i+batch_size].to(self.device)\n",
    "        \n",
    "            # pass through VAE\n",
    "            z_mean, z_logvar = self.encode(batch_inputs)        \n",
    "            log_prediction = self.decoder(z_mean)\n",
    "            \n",
    "            # label = p/Z\n",
    "            prediction += [log_prediction.exp()]\n",
    "            \n",
    "            mean_rep += [z_mean]\n",
    "        \n",
    "        prediction = torch.cat(prediction, dim=0)\n",
    "        mean_rep = torch.cat(mean_rep, dim=0)\n",
    "        \n",
    "        max_pos = prediction.argmax(1)\n",
    "        labels = F.one_hot(max_pos, num_classes=self.output_dim)\n",
    "        \n",
    "        state_population = labels.sum(dim=0)\n",
    "        \n",
    "        # save new guess of representative-inputs\n",
    "        representative_inputs=[]\n",
    "        \n",
    "        for i in range(state_population.shape[-1]):\n",
    "            if state_population[i]>0:\n",
    "                if bias == None:\n",
    "                    center_z = ((mean_rep[labels[:,i].bool()]).mean(dim=0)).reshape(1,-1)\n",
    "                else:\n",
    "                    weights = bias[labels[:,i].bool()].reshape(-1,1)\n",
    "                    center_z = ((weights*mean_rep[labels[:,i].bool()]).sum(dim=0)/weights.sum()).reshape(1,-1)\n",
    "                \n",
    "                # find the one cloest to center_z as representative-inputs\n",
    "                dist=torch.square(mean_rep-center_z).sum(dim=-1)                \n",
    "                index = torch.argmin(dist)\n",
    "                representative_inputs+=[inputs[index].reshape(1,-1)]\n",
    "                # print(index)\n",
    "        \n",
    "        representative_inputs = torch.cat(representative_inputs, dim=0)\n",
    "            \n",
    "        return representative_inputs\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def update_labels(self, inputs, batch_size):\n",
    "        if self.UpdateLabel:\n",
    "            labels = []\n",
    "            \n",
    "            for i in range(0, len(inputs), batch_size):\n",
    "                batch_inputs = inputs[i:i+batch_size].to(self.device)\n",
    "            \n",
    "                # pass through VAE\n",
    "                z_mean, z_logvar = self.encode(batch_inputs)        \n",
    "                log_prediction = self.decoder(z_mean)\n",
    "                \n",
    "                # label = p/Z\n",
    "                labels += [log_prediction.exp()]\n",
    "            \n",
    "            labels = torch.cat(labels, dim=0)\n",
    "            max_pos = labels.argmax(1)\n",
    "            labels = F.one_hot(max_pos, num_classes=self.output_dim)\n",
    "            \n",
    "            return labels\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def save_representative_parameters(self, path, index=0):\n",
    "        \n",
    "        # output representative centers\n",
    "        representative_path = path + '_representative_inputs' + str(index) + '.npy'\n",
    "        representative_weight_path = path + '_representative_weight' + str(index) + '.npy'\n",
    "        representative_z_mean_path = path + '_representative_z_mean' + str(index) + '.npy'\n",
    "        representative_z_logvar_path = path + '_representative_z_logvar' + str(index) + '.npy'\n",
    "        os.makedirs(os.path.dirname(representative_path), exist_ok=True)\n",
    "        \n",
    "        np.save(representative_path, self.representative_inputs.cpu().data.numpy())\n",
    "        np.save(representative_weight_path, self.representative_weights(self.idle_input).cpu().data.numpy())\n",
    "        \n",
    "        representative_z_mean, representative_z_logvar = self.get_representative_z()\n",
    "        np.save(representative_z_mean_path, representative_z_mean.cpu().data.numpy())\n",
    "        np.save(representative_z_logvar_path, representative_z_logvar.cpu().data.numpy())\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def save_traj_results(self, inputs, batch_size):\n",
    "        all_prediction=[] \n",
    "        all_z_sample=[] \n",
    "        all_z_mean=[] \n",
    "        \n",
    "        for i in range(0, len(inputs), batch_size):\n",
    "            \n",
    "            batch_inputs = inputs[i:i+batch_size].to(self.device)\n",
    "        \n",
    "            # pass through VAE\n",
    "            z_mean, z_logvar = self.encode(batch_inputs)\n",
    "            z_sample = self.reparameterize(z_mean, z_logvar)\n",
    "        \n",
    "            log_prediction = self.decoder(z_mean)\n",
    "            \n",
    "            all_prediction+=[log_prediction.exp().cpu()]\n",
    "            all_z_sample+=[z_sample.cpu()]\n",
    "            all_z_mean+=[z_mean.cpu()]\n",
    "            \n",
    "        all_prediction = torch.cat(all_prediction, dim=0)\n",
    "        all_z_sample = torch.cat(all_z_sample, dim=0)\n",
    "        all_z_mean = torch.cat(all_z_mean, dim=0)\n",
    "        \n",
    "        max_pos = all_prediction.argmax(1)\n",
    "        labels = F.one_hot(max_pos, num_classes=self.output_dim)\n",
    "        \n",
    "        # save the fractional population of different states\n",
    "        population = torch.sum(labels,dim=0).float()/len(inputs)\n",
    "        return all_z_mean, all_z_sample, labels, population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPIB_zmean(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_type, z_dim, output_dim, data_shape, device, UpdateLabel= False, neuron_num1=128, \n",
    "                 neuron_num2=128):\n",
    "        \n",
    "        super(SPIB_zmean, self).__init__()\n",
    "        if encoder_type == 'Nonlinear':\n",
    "            self.encoder_type = 'Nonlinear'\n",
    "        else:\n",
    "            self.encoder_type = 'Linear'\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.neuron_num1 = neuron_num1\n",
    "        self.neuron_num2 = neuron_num2\n",
    "        \n",
    "        self.data_shape = data_shape\n",
    "        \n",
    "        self.UpdateLabel = UpdateLabel\n",
    "        \n",
    "        self.eps = 1e-10\n",
    "        self.device = device\n",
    "        \n",
    "        \n",
    "        self.encoder = self._encoder_init()\n",
    "\n",
    "        if self.encoder_type == 'Nonlinear': \n",
    "            self.encoder_mean = nn.Linear(self.neuron_num1, self.z_dim)\n",
    "        else:\n",
    "            self.encoder_mean = nn.Linear(np.prod(self.data_shape), self.z_dim)\n",
    "\n",
    "                \n",
    "    def _encoder_init(self):\n",
    "        \n",
    "        modules = [nn.Linear(np.prod(self.data_shape), self.neuron_num1)]\n",
    "        modules += [nn.ReLU()]\n",
    "        for _ in range(1):\n",
    "            modules += [nn.Linear(self.neuron_num1, self.neuron_num1)]\n",
    "            modules += [nn.ReLU()]\n",
    "        \n",
    "        return nn.Sequential(*modules)\n",
    "    \n",
    "\n",
    "    \n",
    "    def encode(self, inputs):\n",
    "        enc = self.encoder(inputs)\n",
    "        \n",
    "        if self.encoder_type == 'Nonlinear': \n",
    "            z_mean = self.encoder_mean(enc)\n",
    "        else:\n",
    "            z_mean = self.encoder_mean(inputs)\n",
    "        \n",
    "        return z_mean\n",
    "    \n",
    "    def forward(self, data):\n",
    "        inputs = torch.flatten(data, start_dim=1)\n",
    "        \n",
    "        z_mean = self.encode(inputs)\n",
    "        \n",
    "        return z_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "default_device = torch.device(\"cpu\")\n",
    "\n",
    "t0 = 0\n",
    "RC_dim = 2\n",
    "encoder_type = 'Nonlinear'\n",
    "neuron_num1 = 16\n",
    "neuron_num2 = 16\n",
    "batch_size = 500\n",
    "threshold = 0.03\n",
    "patience = 2\n",
    "refinements = 10\n",
    "log_interval = 10000\n",
    "lr_scheduler_step_size = 1\n",
    "lr_scheduler_gamma = 1\n",
    "learning_rate = 0.001\n",
    "beta = 0.001\n",
    "\n",
    "#load files to plot initial states\n",
    "dcom = np.loadtxt('d_all.dat') #D_com values of all frames\n",
    "initial_label_np = np.loadtxt('label_all.dat') # initial state labels of all frames\n",
    "\n",
    "output_dim = initial_label_np.shape[1]\n",
    "\n",
    "#data_all.dat contains descriptor vlaues of all frames\n",
    "traj_data_np = np.loadtxt('data_all.dat')\n",
    "traj_data = torch.from_numpy(traj_data_np).float().to(device)\n",
    "\n",
    "#label_all_dt.dat contains the initial labels after time dt\n",
    "initial_label_dt = np.loadtxt('label_all_dt.dat')\n",
    "traj_labels_dt = torch.from_numpy(initial_label_dt).float().to(device)\n",
    "\n",
    "#past_data_all_dt.dat contains the descriptor values at t=0\n",
    "past_traj_data = np.loadtxt('past_data_all_dt.dat')\n",
    "past_traj_data = torch.from_numpy(past_traj_data).float().to(device)\n",
    "\n",
    "#future_data_all_dt.dat contains the descriptor values at t=dt\n",
    "future_traj_data = np.loadtxt('future_data_all_dt.dat')\n",
    "future_traj_data = torch.from_numpy(future_traj_data).float().to(device)\n",
    "\n",
    "traj_weights = None\n",
    "\n",
    "seed = 0\n",
    "UpdateLabel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d = dcom\n",
    "data_t = traj_data_np[:,16]*57.2958\n",
    "\n",
    "labels=initial_label_np\n",
    "\n",
    "hist=plt.hist2d(data_d,data_t,bins=100)\n",
    "\n",
    "state_num=labels.shape[1]\n",
    "state_labels=np.arange(state_num)\n",
    "\n",
    "hist_state=np.zeros([state_num]+list(hist[0].shape))\n",
    "\n",
    "for i in range(state_num):\n",
    "    hist_state[i]=plt.hist2d(data_d,data_t,bins=[hist[1],hist[2]],weights=labels[:,i])[0]\n",
    "    \n",
    "label_map50=np.argmax(hist_state,axis=0).astype(float)\n",
    "label_map50[hist[0]==0]=np.nan\n",
    "\n",
    "plt.close()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "fmt = matplotlib.ticker.FuncFormatter(lambda x, pos: state_labels[x])\n",
    "tickz = np.arange(0,len(state_labels))\n",
    "\n",
    "cMap = c.ListedColormap(plt.cm.tab20.colors[0:16])\n",
    "im=ax.pcolormesh(hist[1], hist[2], label_map50.T, cmap=cMap, vmin=-0.5, vmax=len(state_labels)-0.5)\n",
    "cb1 = fig.colorbar(im,ax=ax,format=fmt, ticks=tickz)\n",
    "\n",
    "plt.xlabel(\"D$_{com}$\")\n",
    "plt.ylabel(\"Torsional angle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape, train_past_data, train_future_data, train_data_labels, train_data_weights, \\\n",
    "        test_past_data, test_future_data, test_data_labels, test_data_weights = \\\n",
    "            data_init(traj_labels_dt, past_traj_data, future_traj_data, traj_weights)\n",
    "\n",
    "IB = SPIB(encoder_type, RC_dim, output_dim, data_shape, device, \\\n",
    "                   UpdateLabel, neuron_num1, neuron_num2)\n",
    "IB.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "IB.init_representative_inputs(train_past_data, train_data_labels)\n",
    "train_result = train(IB, beta, train_past_data, train_future_data, \\\n",
    "                                       train_data_labels, train_data_weights, test_past_data, test_future_data, \\\n",
    "                                           test_data_labels, test_data_weights, learning_rate, lr_scheduler_step_size, lr_scheduler_gamma,\\\n",
    "                                               batch_size, threshold, patience, refinements, \\\n",
    "                                                   log_interval, device, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_final_result(IB, device, train_past_data, train_future_data, train_data_labels, train_data_weights, \\\n",
    "                                      test_past_data, test_future_data, test_data_labels, test_data_weights, batch_size, \\\n",
    "                                      beta, learning_rate, seed)\n",
    "final_zmean, final_zsample,final_labels, final_population = IB.save_traj_results(traj_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results\n",
    "np.savetxt(\"final_zmean_spib.dat\", final_zmean.detach().numpy(),fmt='%7.6f')\n",
    "np.savetxt(\"final_labels_spib.dat\", final_labels.detach().numpy(),fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d = dcom\n",
    "data_t = traj_data_np[:,16]*57.2958\n",
    "labels=final_labels.detach().numpy()\n",
    "\n",
    "hist=plt.hist2d(data_d,data_t,bins=100)\n",
    "\n",
    "state_num=labels.shape[1]\n",
    "state_labels=np.arange(state_num)\n",
    "\n",
    "hist_state=np.zeros([state_num]+list(hist[0].shape))\n",
    "\n",
    "for i in range(state_num):\n",
    "    hist_state[i]=plt.hist2d(data_d,data_t,bins=[hist[1],hist[2]],weights=labels[:,i])[0]\n",
    "    \n",
    "label_map50=np.argmax(hist_state,axis=0).astype(float)\n",
    "label_map50[hist[0]==0]=np.nan\n",
    "\n",
    "plt.close()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "fmt = matplotlib.ticker.FuncFormatter(lambda x, pos: state_labels[x])\n",
    "tickz = np.arange(0,len(state_labels))\n",
    "\n",
    "cMap = c.ListedColormap(plt.cm.tab20.colors[0:16])\n",
    "im=ax.pcolormesh(hist[1], hist[2], label_map50.T, cmap=cMap, vmin=-0.5, vmax=len(state_labels)-0.5)\n",
    "cb1 = fig.colorbar(im,ax=ax,format=fmt, ticks=tickz)\n",
    "\n",
    "plt.xlabel(\"D$_{com}$\")\n",
    "plt.ylabel(\"Torsional angle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= final_zmean.detach().numpy()\n",
    "labels=final_labels.detach().numpy()\n",
    "\n",
    "hist=plt.hist2d(data[:,0],data[:,1],bins=100)\n",
    "\n",
    "state_num=labels.shape[1]\n",
    "state_labels=np.arange(state_num)\n",
    "\n",
    "hist_state=np.zeros([state_num]+list(hist[0].shape))\n",
    "\n",
    "for i in range(state_num):\n",
    "    hist_state[i]=plt.hist2d(data[:,0],data[:,1],bins=[hist[1],hist[2]],weights=labels[:,i])[0]\n",
    "    \n",
    "label_map50=np.argmax(hist_state,axis=0).astype(float)\n",
    "label_map50[hist[0]==0]=np.nan\n",
    "\n",
    "plt.close()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "fmt = matplotlib.ticker.FuncFormatter(lambda x, pos: state_labels[x])\n",
    "tickz = np.arange(0,len(state_labels))\n",
    "\n",
    "cMap = c.ListedColormap(plt.cm.tab20.colors[0:16])\n",
    "im=ax.pcolormesh(hist[1], hist[2], label_map50.T, cmap=cMap, vmin=-0.5, vmax=len(state_labels)-0.5)\n",
    "cb1 = fig.colorbar(im,ax=ax,format=fmt, ticks=tickz)\n",
    "\n",
    "\n",
    "plt.xlabel(\"CV1\")\n",
    "plt.ylabel(\"CV2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model_name = \"model\"\n",
    "tr_folder=model_name+\"/\"\n",
    "!mkdir -p \"{tr_folder}\"\n",
    "\n",
    "torch.save(IB.state_dict(), tr_folder+model_name+'_state_dict.pt')\n",
    "\n",
    "fake_data = traj_data[0].unsqueeze(0)\n",
    "IB2 = SPIB_zmean(encoder_type, RC_dim, output_dim, data_shape, device, \\\n",
    "                   UpdateLabel, neuron_num1, neuron_num2)\n",
    "IB2.to(device)\n",
    "params = IB.named_parameters()\n",
    "params2 = IB2.named_parameters()\n",
    "dict_params2 = dict(params2)\n",
    "for name, param in params:\n",
    "    if name in dict_params2:\n",
    "        dict_params2[name].data.copy_(param.data)\n",
    "        \n",
    "# save model\n",
    "mod2 = torch.jit.trace(IB2, fake_data)\n",
    "mod2.save(tr_folder+model_name+\"_zmean.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
